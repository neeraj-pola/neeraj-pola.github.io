---
layout: post
title: Retrieval-Augmente Generation (Draft)
thumbnail: "/images/CTC/thumbnail.mp4"
---

***
You must have heard about this term booming all across the feilds of AI.

Well does the hype for it lives upto the expectations of its impact? Yess!! ofcourse, it does, and we will deep dive into its core functionality by familirizing with each of its componenets piece by piece.

Before that we will look into how an llm generates an output without RAG to understand the need for it.

![_config.yml]({{ site.baseurl }}/images/RAG/vanilla_LLM.png)

Meanwhile on June 14, 2025, South Africa just chased down 282 at Lord’s to get their first icc cup ever.

You can see the problem here, without RAG, the LLM is not able to give the information that is up to date, to fix these sort of issues we use RAG.

We can categorize this RAG into 3 types:

i. Naive RAG

ii. Advanced RAG

iii. Modular RAG

Let us get into the Naive RAG, it follows a traditional process that includes indexing, retrieval and generation

![_config.yml]({{ site.baseurl }}/images/RAG/Naive_RAG.png)

we will dive deep into each of these components.

![_config.yml]({{ site.baseurl }}/images/RAG/indexing_naive_rag.png)

The **source**(documents) can be of any text type (optimizingly using these sources will be discussed later), is then passed to a stage **formatting**, here texts from different sources are combined, then cleaned and are normalized so that all the text is converted to uniform plain text format, this step is crucial because it does help to remove any disrepancy in wordings (CAT, Cats, cat are considered same), this formatted text is then passed to **chunking** where the text is split into fixed sizes, this chunks are chunks then passed into an embedidng model (sounds fancy but it is just a model which converts text to numbers so that the machines can interpret it more easily), now we have **vector representations** of those chunks, these vectors are stored in a vector database (optimizing this database for faster processing is discussed later).

![_config.yml]({{ site.baseurl }}/images/RAG/retrieval_naive_rag.png)

Now we will go into the retrieval phase, where the user's query is recieved which passes into the same embedding model that was used before (for coherent flow) which generates a query vector that is compared with the vector database (previously used to store the chunks) using some similarity scores, based on the scores, some documents are selected these can be also called as tok-k chunks.

![_config.yml]({{ site.baseurl }}/images/RAG/generation_naive_rag.png)

Then the intiall query along with out selected documents is given to our LLM, which either uses them as the base to generate a output or uses its inherent knowledge (the parameters that it was initially trained on) to generate an out (it generally depends on the tasks). 

And that is the basis of the Naive RAG. So far so good, but now we will see the need for other architectures, some drawbacks of the Naive RAG:

1. **Retrieval Challenges** : When similarity scoring is off, the system retrieves things that look kind of right  but miss the intent, say when the query is about a fruit 'apple' but it finds chunks like 'apple products' to be more silmilar than the fruit itself.

2. **Gnerational Difficulties** : When the model starts to **halucinate** and just blindly relies on its previous knowledge without using the help of top-k chunks, this may cause the model to give outputs that are outdatated.

3. **When k is just 1**: when only one chunck is retirieved by the similarity scores, it may not be adequate for the model to acquire context information.

4. **Over rely on top-k chunks**: sometimes the model may just give the chunks as an output, facing a critical 'overfitting' issue.

To overcome this, Advanced RAG is introduced, it mostly focuses on improving retrieval phase and optimizing the indexing.

![_config.yml]({{ site.baseurl }}/images/RAG/advanced_rag.png)

first we will go into how is indexing is optimized with different techniques

![_config.yml]({{ site.baseurl }}/images/RAG/techniques_optimized_indexing.png)

**1. Data granuarity** : instead of creacting chunks of constant sizes (which is done in Naive Rag) we used chunking with differnt sizes to create meaningfull structural chunks. In constant numbered chunking sometimes headings, passages, keypoints from documennts are either chunked by paritally or totally goruping all of them, where as in data granularity differnt structures (headings, passages, keypoints..) are chunked uniquely.

**2. Optimizing Index Structures**: 

In Naive RAG, query indexes are directly compared for similarity check with all the dense vectors in the vector database resulting in retrieving partiallly knowleged chunks (that can only understand the semantic nature but not the syntactic nature) with slower retirieval rate. So we use different optimized techniques like hybrid indexing and Hirearchial indexing.

**A. Hybrid indexing**: It uses combination of both sparse (syntactic) and dense (semantic) vecotrs.
**B. Hirearchial Indexing**: It uses advanced approaches like memory trees, where the first retrieval process is drilled to documents or high-level sections and then drilled down into chunks within them.

>Assume you are in a library with books all over the racks with no proper indications, can you search down for a book (that you properly don't know the name of but know what genre it is and what it contains)? Sounds impossible right? Now say you know the book name, although you have to search all the racks one by one which takes a huge amount of time, now this is what happening in the general naive RAG indeixng, where the books are the vector chunks, query is your book, and racks in the library is a vector database.
>
> Now imagine a library, where two super smart librarians helping you. One listens to your explanation and instantly understands what you mean (semantic match).
The other scans for exact words or phrases you mention (syntactic match). Now these 2 librarians know what you are looking for, but it would be impractical for them to search it for all over the racks (which are unorganized and all the books are randomly placed), thus to prevent from that, they organize their racks in a way where every genre (or rather any classifiable thing) is placed systematically, so that they can narrow iit down in an optimal way.
>
> This is how, combination of Data granularity and Optimzing idexing helps in RAGS as well.

**3. Adding Metadata**: Adding metadata in RAG is like attaching tags or labels to vector chunks (something like a dictionary with keywords along with their values), which can be used to filter the results (like finding all the books by a certain author),and it also supports the hybrid indexing.

**4. Alignment Optimization**: It’s to ensure that every chunk in your index is actually useful and relevant to likely queries, not just match because it mentions certain terms.

> Assume walking back into library asking how transformers work. In a naive RAG setup, the librarian hands you three pages: one says “Transformers are popular,” another lists “BERT, GPT,” and a third starts mid-sentence with “…due to self-attention.” They all mention transformers but none actually explain them. In a better library with alignment optimization, the pages were pre-checked. The librarian gives you a clear, meaningful information, something you can actually learn from.

**5. Mixed Retrieval**: Similar to Hybrid indexing which helps to organize the vector chunks, Mixed Retrival uses both of these informations to give an more impactful output.

> Assume you have two friends one has a sharp memory for names, dates, and exact terms (syntactic), while the other is great at understanding the big picture, t“what” and “why” of things (semantic). You ask them about transformers. The first friend gives you precise terms like “self-attention” and “positional encoding”  but struggles to explain how it all fits together. The second friend walks you through how transformers work, but doesn’t use the exact terminology. Only when you combine both the detailed terms and the conceptual explanation, you get a full, meaningful answer. That’s exactly what Mixed Retrieval does, it blends keyword-based (sparse) and meaning-based (dense) retrieval to give you the most complete and useful context possible.

Great, now let us move with pre-retrieval techniques in the Advanced RAG:

![_config.yml]({{ site.baseurl }}/images/RAG/pre_retrieval_rag.png)

**Query Routing**: It is process of deciding where a query should go based on its type, topic, or intent, before performing retrieval. This helps the RAG to not treat all the queries similarly, well how does that help? The vectors can be from differnt data sources (it can be from either sql database, vector database, or even with an api call), because of this large data it is inefficient to send all the queries to all the data sources, having a routing mechanism helps to navigate their correct destination quickly.

> Assume, you are back to that same library again, without a help desk, you want to read a book of specific author in fiction, although you have got sorted with finding where the section is (thanks to our index optimization techniques), there would still be a lot of rocks for that section which is cumbersome to search for, now say you have an help desk, he would easily navigate you to that particular rack instantly.

**Query Rewritin & Query Expansion**: 
<!-- added new video -->
<video muted loop autoplay controls style="max-width: 100%">
    <source src="/images/RAG/query_rewriting and expansion.mp4" type="video/mp4">
</video>

Once the relevant context is gathered with the help of processing our queries, we will have a numerous vectors retrieved from the vector database, the number can sometimes be so huge, that when we give all of this retrieved vectors to the frozen modell (our LLM) it may get overloaded with that much of information, resulting in an abysmal performance, so what do we do now? we can process the retrieved vectors called Post Retrieval process.


<!-- insert post-retireval RAG -->
**Post-Retrieval RAG:** Here, instead of giving all retrieved information at once, we try to rank and summarize the vectors.









